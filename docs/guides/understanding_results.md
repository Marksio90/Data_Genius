Understanding Results â€” jak czytaÄ‡ wyniki w DataGenius PRO

Ten przewodnik pomaga szybko zinterpretowaÄ‡ to, co widzisz po EDA, przetwarzaniu, treningu modeli, wyjaÅ›nialnoÅ›ci i monitoringu w DataGenius PRO.

1) Co generuje platforma

EDA (Exploratory Data Analysis) â€” przeglÄ…d danych, braki, outliery, korelacje, rozkÅ‚ady (interaktywne wykresy).

Przetwarzanie (Pipeline) â€” imputacja, skalowanie, kodowanie, nazwy cech po transformacjach.

Trening modeli (AutoML) â€” porÃ³wnanie i tuning modeli, wybÃ³r najlepszego.

Ewaluacja â€” metryki jakoÅ›ci + podsumowania (klasyfikacja/regresja).

WyjaÅ›nialnoÅ›Ä‡ â€” waÅ¼noÅ›Ä‡ cech, SHAP (globalnie i lokalnie).

Raporty â€” HTML/PDF/Markdown.

Monitoring â€” drift danych/pojÄ™Ä‡ i spadek jakoÅ›ci (PSI, KS, JS, spadek metryk).

2) EDA â€” na co patrzeÄ‡ w pierwszej kolejnoÅ›ci
ğŸ“Š PrzeglÄ…d danych

Liczba wierszy/kolumn, rozmiar (MB).

RozkÅ‚ad typÃ³w (numeric, categorical, datetime, text/id).

ğŸ” Braki danych

Suma i % brakÃ³w per kolumna.

Heurystyka: jeÅ›li kolumna ma > 50% brakÃ³w (staÅ‚a w projekcie MISSING_DATA_THRESHOLD=0.5) â€” rozwaÅ¼ usuniÄ™cie lub zaawansowanÄ… imputacjÄ™.

W raporcie: tabela â€Kolumny z brakujÄ…cymi danymiâ€ i sugerowana strategia.

âš ï¸ Outliery

Boxploty + liczba outlierÃ³w.

Gdy outlierÃ³w jest bardzo duÅ¼o, rozwaÅ¼ robust scalery, winsoryzacjÄ™ lub transformacjÄ™.

ğŸ”— Korelacje

Macierz korelacji (dla cech numerycznych).

Alerty â€silnych korelacjiâ€ (|r| > 0.8). Uwaga na wspÃ³Å‚liniowoÅ›Ä‡ â€” moÅ¼e psuÄ‡ modele liniowe.

DziaÅ‚ania:

Ustandaryzuj nazwy kolumn (narzÄ™dzie â€clean_column_namesâ€).

SprawdÅº zmiennÄ… docelowÄ… (balans klas, rozkÅ‚ad wartoÅ›ci).

Zanotuj kolumny podejrzane: duÅ¼o brakÃ³w / zerowa wariancja / bardzo wysoka kardynalnoÅ›Ä‡.

3) Przetwarzanie (Pipeline)
Imputacja

Numeryczne: domyÅ›lnie medianÄ… (SimpleImputer), opcjonalnie KNN.

Kategoryczne: najczÄ™stszÄ… wartoÅ›ciÄ… lub drop wierszy (jeÅ›li jawnie wybrano).

Braki w target â†’ wiersze sÄ… usuwane (log ostrzeÅ¼eÅ„).

Skalowanie & kodowanie

Numeryczne: StandardScaler (domyÅ›lnie).

Kategoryczne: OneHotEncoder (handle_unknown='ignore'), peÅ‚na lista cech po OHE dostÄ™pna w feature_names.

InÅ¼ynieria cech

Daty â†’ *_year, *_month, *_day, *_dayofweek, *_quarter, *_is_weekend.

Interakcje wybranych cech (mnoÅ¼enia) i polynomial (kwadraty, sqrt).

Binning numerycznych (kwantyle).

Na co uwaÅ¼aÄ‡:
DuÅ¼o nowych cech âŸ¶ ryzyko przeuczenia. Kontroluj listÄ™ features_created i porÃ³wnuj metryki na walidacji krzyÅ¼owej.

4) Ewaluacja â€” klasyfikacja

Metryka â€best_scoreâ€ w systemie to Accuracy (domyÅ›lnie w ModelEvaluator), ale patrz szerzej:

Accuracy â€” odsetek poprawnych klasyfikacji. WraÅ¼liwy na imbalance.

Precision / Recall / F1 (weighted) â€” lepsze przy niezbalansowanych klasach.

ROC AUC (dla 2 klas) â€” jakoÅ›Ä‡ rankingowa progÃ³w.

Macierz pomyÅ‚ek â€” pokaÅ¼e, gdzie model siÄ™ myli (FN vs FP).

PR AUC (jeÅ›li dostÄ™pne) â€” lepsze przy rzadkich klasach pozytywnych.

Dobre praktyki:

Gdy klasa pozytywna jest rzadka, kieruj siÄ™ Recall/PR AUC, nie Accuracy.

Kalibracja progu: czasem 0.50 nie jest optymalny (wykres ROC/PR).

Zawsze porÃ³wnuj do baseline (np. â€zawszepoziom wiÄ™kszoÅ›ciâ€).

5) Ewaluacja â€” regresja

â€Best_scoreâ€ w projekcie to RÂ². Dodatkowo sprawdzaj:

MAE â€” Å›redni bÅ‚Ä…d bezwzglÄ™dny (Å‚atwy do interpretacji w jednostkach).

RMSE â€” karze wiÄ™ksze bÅ‚Ä™dy (>= MSE^0.5).

MAPE â€” % bÅ‚Ä…d (uwaga: wymaga wartoÅ›ci > 0).

RÂ² â€” dopasowanie (1 = idealnie, < 0 gorzej niÅ¼ baseline).

Dobre praktyki:

Patrz na residua: losowe â‰ˆ OK; struktura â‰ˆ brak liniowoÅ›ci lub feature drift.

PorÃ³wnaj do baseline (np. Å›rednia/mediana targetu).

6) WyjaÅ›nialnoÅ›Ä‡ â€” feature importance & SHAP

Feature importance:

Drzewa: feature_importances_

Liniowe: |coef|

Permutacja: spadek metryki po permutacji cechy

SHAP:

Globalnie: wykres summary â€” ktÃ³re cechy majÄ… najwiÄ™kszy wpÅ‚yw i w jakim kierunku.

Lokalnie: dla konkretnej obserwacji â€” waterfall/force plot (dlaczego taka predykcja).

Wyniki w aplikacji:

top_features â€” 5 najwaÅ¼niejszych cech.

Insight: â€Top 3 cechy stanowiÄ… X% caÅ‚kowitej waÅ¼noÅ›ciâ€ âŸ¶ model zaleÅ¼ny od niewielu zmiennych (ryzyko niestabilnoÅ›ci).

7) WybÃ³r najlepszego modelu

ModelTrainer (PyCaret) porÃ³wnuje zestaw modeli (wg strategii z config/model_registry.py), opcjonalnie tuning.

ModelEvaluator zapisuje:

metrics (klasyfikacja/regresja),

best_model_name,

best_score (Accuracy lub RÂ², w zaleÅ¼noÅ›ci od problemu).

ModelExplainer dodaje feature_importance + shap_values (o ile moÅ¼liwe).

Uwaga: JeÅ›li TwÃ³j przypadek biznesowy wymaga innej metryki (np. Recall/F1/MAE), potraktuj best_score jako wskaÅºnik ogÃ³lny, a decyzjÄ™ podejmuj wg metryki biznesowej.

8) Raporty

Raport (HTML/PDF/MD) zawiera:

PrzeglÄ…d danych (rozmiar, liczba kolumn),

Statystyki (num/kategoria, sparsity),

Braki/outliery/korelacje,

Podsumowanie (kluczowe wnioski i rekomendacje).

Tipy:

HTML jest najbogatszy wizualnie.

PDF generujemy przez konwersjÄ™ HTML (fallback do HTML gdy brak weasyprint).

Markdown â€” lekki, dobry do PR/README.

9) Monitoring i drift

Metryki driftu (domyÅ›lne progi w config/constants.py):

PSI (Population Stability Index) â€” > 0.1 ryzyko driftu.

KS (Kolmogorovâ€“Smirnov) â€” > 0.05 sygnaÅ‚ rÃ³Å¼nicy rozkÅ‚adÃ³w.

JS (Jensenâ€“Shannon) â€” > 0.1 sygnaÅ‚ rÃ³Å¼nicy rozkÅ‚adÃ³w.

Spadek jakoÅ›ci: jeÅ›li metryka modelu spadnie o â‰¥ 5% (PERFORMANCE_THRESHOLD=0.05) wzglÄ™dem referencji â€” rozwaÅ¼ akcjÄ™.

Reakcje na problemy:

Data drift (wejÅ›cia): zaktualizuj pipeline/enkodery, sprawdÅº dystrybucje.

Concept drift (relacja Xâ†’y): zwiÄ™ksz wagÄ™ nowszych danych, retrain.

Spadek jakoÅ›ci: tuning, kalibracja progu, dodanie nowych cech lub retraining wg retraining_scheduler.py.

10) Szybka Å›ciÄ…ga: â€co robiÄ‡, gdyâ€¦â€

Niezbalansowane klasy â†’ uÅ¼yj Recall/F1/PR AUC; rozwaÅ¼ wagi klas/oversampling; ustaw prÃ³g decyzyjny.

Wysoka korelacja cech â†’ usuÅ„ nadmiarowe, uÅ¼yj modeli odpornych (drzewa/boostingi) lub regularizacji (L1/L2).

DuÅ¼o brakÃ³w â†’ imputacja kierowana dziedzinÄ…, ewentualnie usuwaj kolumny >50% brakÃ³w.

Outliery psujÄ… metryki â†’ robust scalery, winsoryzacja, modele odporne (MAE, drzewiaste).

RMSE duÅ¼e, RÂ² niskie â†’ sprawdÅº cechy nieliniowe/interakcje lub boosting.

Model â€czarna skrzynkaâ€ â†’ uÅ¼yj explainerÃ³w (SHAP), rozwaÅ¼ model prostszy dla governance.

11) FAQ

Q: Dlaczego mÃ³j â€best modelâ€ nie maksymalizuje mojej metryki biznesowej?
A: best_score to Accuracy/RÂ² by default. WeÅº pod uwagÄ™ metrykÄ™ biznesowÄ… i w razie potrzeby wybierz inny model rÄ™cznie.

Q: SHAP nie dziaÅ‚a dla mojego modelu.
A: NiektÃ³re estymatory nie sÄ… wspierane â€out-of-the-boxâ€. UÅ¼yj permutation importance lub rozwaÅ¼ model kompatybilny.

Q: Kiedy retrenowaÄ‡?
A: Gdy PSI/KS/JS przekraczajÄ… progi lub spadek metryk â‰¥ 5% wzglÄ™dem referencji â€” zgodnie z logikÄ… w moduÅ‚ach monitoringu.

12) SÅ‚owniczek mini

Accuracy â€” (TP+TN)/(TP+TN+FP+FN)

Precision â€” TP/(TP+FP)

Recall â€” TP/(TP+FN)

F1 â€” 2Â·(PrecÂ·Rec)/(Prec+Rec)

MAE/MSE/RMSE â€” miary bÅ‚Ä™du dla regresji

RÂ² â€” stopieÅ„ wyjaÅ›nionej wariancji

PSI, KS, JS â€” wskaÅºniki driftu rozkÅ‚adÃ³w

WskazÃ³wka koÅ„cowa: patrz na caÅ‚oÅ›Ä‡ obrazu â€” EDA â†’ poprawki danych â†’ przetwarzanie â†’ wÅ‚aÅ›ciwa metryka â†’ interpretacja â†’ monitoring. JeÅ›li chcesz, AI Mentor streÅ›ci wyniki w jÄ™zyku nietechnicznym i podsunie konkretne dziaÅ‚ania.