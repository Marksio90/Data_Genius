# 06_ğŸ“_AI_Mentor.py
"""
DataGenius PRO â€” AI Mentor (PRO+++)
Interaktywny mentor projektu: Q&A o EDA/ML/metrics/next-steps, z kontekstem aplikacji.
"""

from __future__ import annotations

import os
import sys
import time
import math
import json
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st

warnings.filterwarnings("ignore")

# === NAZWA_SEKCJI === Bootstrapping Å›cieÅ¼ek ===
ROOT_DIR = Path(__file__).parent.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

# === NAZWA_SEKCJI === Importy ekosystemu (UI + Core) ===
try:
    from frontend.app_layout import render_header, render_error, render_success, render_warning
except Exception:
    def render_header(title: str, subtitle: str = "") -> None:
        st.header(title)
        if subtitle: st.caption(subtitle)
    def render_error(title: str, detail: Optional[str] = None) -> None:
        st.error(title + (f": {detail}" if detail else ""))
    def render_success(msg: str) -> None:
        st.success(msg)
    def render_warning(msg: str) -> None:
        st.warning(msg)

try:
    from core.state_manager import get_state_manager
except Exception:
    get_state_manager = None  # defensywnie


# === NAZWA_SEKCJI === Page Config (bezpiecznie) ===
try:
    st.set_page_config(page_title="ğŸ“ AI Mentor â€” DataGenius PRO+++", page_icon="ğŸ“", layout="wide")
except Exception:
    pass

# === NAZWA_SEKCJI === Logger (lekki) ===
try:
    from src.utils.logger import get_logger  # opcjonalnie
    log = get_logger(__name__)
except Exception:
    import logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    log = logging.getLogger("ai_mentor")

# === NAZWA_SEKCJI === Security Gate (ONLINE/OFFLINE) ===

def _get_openai_key() -> Optional[str]:
    try:
        key = st.secrets.get("OPENAI_API_KEY")  # type: ignore[attr-defined]
        if key: return key
    except Exception:
        pass
    if st.session_state.get("openai_api_key"):  # ustawiane np. na Home
        return st.session_state.get("openai_api_key")
    return os.environ.get("OPENAI_API_KEY")

def _ai_status_chip(online: bool) -> None:
    st.markdown(
        f"""
        <div style="text-align:right;">
          <span style="display:inline-block;padding:2px 10px;border-radius:999px;
                       background:{'#15a34a1a' if online else '#f59e0b1a'};
                       border:1px solid {'#16a34a' if online else '#f59e0b'};
                       color:{'#16a34a' if online else '#b45309'};
                       font-weight:600;font-size:12px">
            AI: {'ONLINE' if online else 'OFFLINE'}
          </span>
        </div>
        """,
        unsafe_allow_html=True,
    )

# === NAZWA_SEKCJI === Kontekst projektu (snapshot) ===

def _data_kpis(df: Optional[pd.DataFrame]) -> Dict[str, Any]:
    if df is None or df.empty:
        return {"rows": 0, "cols": 0, "mem_mb": 0.0, "miss_pct": 0.0}
    rows = int(len(df)); cols = int(df.shape[1])
    mem_mb = float(df.memory_usage(deep=True).sum() / (1024**2))
    miss_pct = float(df.isna().sum().sum() / max(1, rows * cols) * 100.0)
    return {"rows": rows, "cols": cols, "mem_mb": round(mem_mb, 2), "miss_pct": round(miss_pct, 2)}

def _safe_get(d: Dict[str, Any], key: str, default: Any = None) -> Any:
    try:
        return d.get(key, default) if isinstance(d, dict) else default
    except Exception:
        return default

def _build_context_snapshot() -> Dict[str, Any]:
    """
    Tworzy zwiÄ™zÅ‚y snapshot (bez surowych danych) do wstrzykniÄ™cia w system prompt Mentora.
    """
    state = get_state_manager()() if callable(get_state_manager) else None

    df = state.get_data() if state and hasattr(state, "get_data") and state.has_data() else st.session_state.get("raw_df")
    kpi = _data_kpis(df)
    target = state.get_target_column() if state and hasattr(state, "get_target_column") else st.session_state.get("target_column")
    problem = state.get_problem_type() if state and hasattr(state, "get_problem_type") else st.session_state.get("problem_type")

    eda_results = st.session_state.get("eda_results")
    ml_training = st.session_state.get("ml_training", {})          # {"table": df, "report": dict}
    model_comp  = st.session_state.get("model_comparison", {})     # {"table": df, "meta": dict}
    fi_bundle   = st.session_state.get("feature_importance", {})   # {"table": df, "meta": dict}
    pipeline    = st.session_state.get("pipeline_state", {})

    # SkrÃ³ty EDA
    eda_short: Dict[str, Any] = {}
    try:
        if eda_results and "eda_results" in eda_results:
            eda = eda_results["eda_results"]
            stats = _safe_get(eda.get("StatisticalAnalyzer", {}), "overall", {})
            miss = _safe_get(eda.get("MissingDataAnalyzer", {}), "summary", {})
            eda_short = {
                "n_numeric": stats.get("n_numeric"),
                "n_categorical": stats.get("n_categorical"),
                "missing_pct": miss.get("missing_percentage"),
                "n_columns_missing": miss.get("n_columns_with_missing"),
            }
    except Exception:
        pass

    # Model summary
    report = _safe_get(ml_training, "report", {})
    best_key = _safe_get(report, "best_key") or _safe_get(_safe_get(model_comp, "meta", {}), "best_key")
    primary_metric = _safe_get(report, "primary_metric") or _safe_get(_safe_get(model_comp, "meta", {}), "scoring")
    test_metrics = _safe_get(report, "test_metrics", {})

    # Top FI (do 15)
    fi_df = _safe_get(fi_bundle, "table")
    top_fi: List[Dict[str, Any]] = []
    if isinstance(fi_df, pd.DataFrame) and not fi_df.empty and "feature" in fi_df.columns:
        view = fi_df.copy().head(15)
        score_col = "importance_norm" if "importance_norm" in view.columns else ("importance" if "importance" in view.columns else None)
        for _, r in view.iterrows():
            top_fi.append({"feature": str(r["feature"]), "score": float(r.get(score_col)) if score_col else None})

    # Pipeline quick
    pipe_short = {"status": pipeline.get("status"), "stage": pipeline.get("stage"), "n_stages": len(pipeline.get("stages", []))} if pipeline else {}

    return {
        "project_overview": {
            "kpi": kpi,
            "target": target,
            "problem": problem,
        },
        "eda": eda_short,
        "model": {
            "best_key": best_key,
            "primary_metric": primary_metric,
            "test_metrics": test_metrics,
        },
        "feature_importance": {"top": top_fi},
        "pipeline": pipe_short,
    }

# === NAZWA_SEKCJI === LLM Client (OpenAI kompat.) + Fallback OFFLINE ===

def _get_llm_client():
    """
    Zwraca klienta OpenAI (chat.completions) jeÅ›li dostÄ™pny, inaczej None.
    """
    api_key = _get_openai_key()
    if not api_key:
        return None
    try:
        import openai  # OpenAI python SDK v1.x
        client = openai.OpenAI(api_key=api_key)
        return client
    except Exception as e:
        log.warning(f"OpenAI client init failed: {e}")
        return None

def _trim_history(messages: List[Dict[str, str]], max_tokens_est: int = 6000) -> List[Dict[str, str]]:
    """
    Bardzo uproszczone przycinanie historii: limit liczby wiadomoÅ›ci.
    (Estymacja tokenÃ³w jest trudna bez tiktoken â€” przycinamy do N ostatnich tur).
    """
    MAX_TURNS = 16  # 16 tur ~ spokojnie < 6-8k tokenÃ³w dla krÃ³tkich promptÃ³w
    if len(messages) <= MAX_TURNS:
        return messages
    # zachowaj system + pierwszÄ… user + ostatnie 14 wpisÃ³w
    head = [messages[0]]
    tail = messages[-(MAX_TURNS-1):]
    return head + tail

def _system_prompt(context: Dict[str, Any]) -> str:
    """
    System prompt mentora â€” styl PRO+++: konkretnie, defensywnie, bez danych wraÅ¼liwych.
    """
    return (
        "JesteÅ› AI Mentorem klasy PRO+++, ekspertem MLOps/Data Science/Streamlit.\n"
        "Zasady: odpowiadaj zwiÄ™Åºle, konkretnie, w punktach. WyjaÅ›niaj metryki, proponuj next-steps, "
        "zawsze wskazuj ryzyka (data leakage, overfitting, data quality). Nie wymyÅ›laj danych, "
        "polegaj na przekazanym kontekÅ›cie. JeÅ›li czegoÅ› brakuje â€” zaproponuj jak to zdobyÄ‡. "
        "Nie ujawniaj PII/surowych wierszy.\n\n"
        f"Kontekst projektu (JSON):\n{json.dumps(context, ensure_ascii=False)}"
    )

def _call_llm(messages: List[Dict[str, str]], model: str, temperature: float, timeout_s: int = 40, max_retries: int = 2) -> str:
    """
    WywoÅ‚anie chat.completions z retry/backoff. Zwraca treÅ›Ä‡ odpowiedzi lub rzuca wyjÄ…tek.
    """
    client = _get_llm_client()
    if client is None:
        raise RuntimeError("Brak klienta LLM (OFFLINE).")

    delay = 1.5
    last_err: Optional[Exception] = None
    for attempt in range(max_retries + 1):
        try:
            with st.spinner("ğŸ¤– AI myÅ›liâ€¦"):
                resp = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    timeout=timeout_s,
                )
            return resp.choices[0].message.content or "Brak treÅ›ci odpowiedzi."
        except Exception as e:
            last_err = e
            time.sleep(delay)
            delay *= 2
    raise last_err or RuntimeError("Nieznany bÅ‚Ä…d LLM")

def _offline_response(user_msg: str, context: Dict[str, Any]) -> str:
    """
    Fallback odpowiedzi OFFLINE â€” reguÅ‚y i gotowe szkielety.
    """
    ov = context.get("project_overview", {})
    kpi = ov.get("kpi", {})
    target = ov.get("target") or "nieustalony"
    problem = ov.get("problem") or "n/d"

    hints = []
    # Propozycje krokÃ³w na podstawie problemu
    if problem == "classification":
        hints += [
            "Zweryfikuj balans klas (class imbalance) â€” rozwaÅ¼ stratified split lub wagi klas.",
            "SprawdÅº metryki: ROC AUC, F1 weighted; unikaj accuracy przy niezbalansowanych danych.",
            "Zweryfikuj waÅ¼ne cechy (FI) i moÅ¼liwy data leakage.",
        ]
    elif problem == "regression":
        hints += [
            "UÅ¼yj metryk: RÂ², RMSE, MAE â€” raportuj wszystkie trzy.",
            "SprawdÅº rozkÅ‚ad residuali i heteroskedastycznoÅ›Ä‡.",
            "Przetestuj transformacje cech (log, Box-Cox) oraz interakcje.",
        ]
    else:
        hints += [
            "Zidentyfikuj najpierw typ problemu (classification vs regression).",
            "Zdefiniuj target i walidacjÄ™.",
        ]

    size_info = f"Dane: {kpi.get('rows','n/d')} wierszy Ã— {kpi.get('cols','n/d')} kolumn; braki ~ {kpi.get('miss_pct','n/d')}%."
    guidance = [
        f"Target: **{target}**, typ problemu: **{problem}**.",
        size_info,
        "Rekomendowane nastÄ™pne kroki:",
        *[f"- {h}" for h in hints[:4]],
        "Skonfiguruj walidacjÄ™: train/val/test z losowaniem powtarzalnym i raportem metryk.",
    ]

    # Proste reguÅ‚y odpowiadania na popularne komendy
    msg = user_msg.lower()
    if "kolejne kroki" in msg or "next step" in msg or "co dalej" in msg:
        return "\n".join(guidance)
    if "metryk" in msg or "metrics" in msg:
        return (
            "DobÃ³r metryk:\n"
            "- **Klasyfikacja**: ROC AUC (priorytet), F1 weighted, Accuracy (ostroÅ¼nie przy niezbalansowanych).\n"
            "- **Regresja**: RMSE (czuÅ‚a na outliers), MAE (odporna), RÂ² (globalne dopasowanie).\n"
            "Raportuj na walidacji i teÅ›cie, z odchyleniem (jeÅ›li CV)."
        )
    if "leak" in msg or "leakage" in msg or "przeciek" in msg:
        return (
            "Data leakage â€” checklist:\n"
            "1) Czy cechy powstajÄ… z docelowej zmiennej lub przyszÅ‚ej informacji?\n"
            "2) Czy preprocessing/skalowanie byÅ‚ dopasowany na caÅ‚ym zbiorze (zamiast tylko na train)?\n"
            "3) Czy target encoding byÅ‚ robiony w ramach CV/splita?\n"
            "4) Czy identyfikatory czasowe sÄ… traktowane poprawnie (time-based split)?"
        )
    return "\n".join(guidance[:3] + [f"- Odpowiadam w trybie OFFLINE â€” podaj bardziej szczegÃ³Å‚owe pytanie, a zasugerujÄ™ konkretne dziaÅ‚ania."])

# === NAZWA_SEKCJI === Utrzymanie historii czatu ===

def _init_chat() -> None:
    st.session_state.setdefault("mentor_chat", [])
    # Inicjalna wiadomoÅ›Ä‡ systemowa nie jest wyÅ›wietlana, ale trzymamy jÄ… w buforze do LLM
    st.session_state.setdefault("mentor_system", _system_prompt(_build_context_snapshot()))

def _append_chat(role: str, content: str) -> None:
    st.session_state["mentor_chat"].append({"role": role, "content": content, "ts": time.time()})
    # limit historii (20 wpisÃ³w user/assistant)
    if len(st.session_state["mentor_chat"]) > 40:
        st.session_state["mentor_chat"] = st.session_state["mentor_chat"][-40:]

def _render_history() -> None:
    for m in st.session_state.get("mentor_chat", []):
        if m["role"] == "user":
            with st.chat_message("user"):
                st.markdown(m["content"])
        else:
            with st.chat_message("assistant"):
                st.markdown(m["content"])

# === NAZWA_SEKCJI === UI GÅÃ“WNE ===

def main() -> None:
    render_header("ğŸ“ AI Mentor", "Zadawaj pytania o swoje dane, EDA, metryki i modele â€” mentor wykorzysta kontekst projektu.")

    _init_chat()
    context = _build_context_snapshot()
    online = _get_openai_key() is not None

    # Panel statusu
    cA, cB, cC = st.columns([0.6, 0.2, 0.2])
    with cA:
        st.caption("Mentor dziaÅ‚a na streszczeniu projektu (bez surowych rekordÃ³w).")
    with cB:
        st.caption("Tryb")
        _ai_status_chip(online)
    with cC:
        st.caption("Kontekst")
        if st.button("ğŸ”„ OdÅ›wieÅ¼ snapshot", use_container_width=True):
            st.session_state["mentor_system"] = _system_prompt(_build_context_snapshot())
            render_success("Zaktualizowano kontekst mentora.")

    # Parametry LLM
    with st.expander("âš™ï¸ Ustawienia AI", expanded=False):
        model = st.text_input("Model", value=os.environ.get("DG_LLM_MODEL", "gpt-4o-mini"))
        temperature = st.slider("Temperatura", min_value=0.0, max_value=1.2, value=0.2, step=0.05)
        st.caption("NiÅ¼sza temperatura = bardziej deterministyczne odpowiedzi.")

    # Szybkie podpowiedzi
    st.markdown("#### ğŸ’¡ Szybkie podpowiedzi")
    p1, p2, p3, p4 = st.columns(4)
    if p1.button("ğŸ§­ Zaproponuj kolejne kroki", use_container_width=True):
        _append_chat("user", "Zaproponuj kolejne kroki w projekcie na podstawie obecnego kontekstu.")
    if p2.button("ğŸ“ WyjaÅ›nij metryki", use_container_width=True):
        _append_chat("user", "WyjaÅ›nij, ktÃ³re metryki sÄ… istotne dla mojego problemu i jak je interpretowaÄ‡.")
    if p3.button("ğŸ”¥ OmÃ³w Feature Importance", use_container_width=True):
        _append_chat("user", "Zinterpretuj top Feature Importance i zasugeruj inÅ¼ynieriÄ™ cech.")
    if p4.button("ğŸ›¡ï¸ SprawdÅº ryzyka", use_container_width=True):
        _append_chat("user", "Podaj checklistÄ™ ryzyk: data leakage, overfitting, jakoÅ›Ä‡ danych, bÅ‚Ä™dy walidacji.")

    # Render historii + input
    _render_history()

    user_msg = st.chat_input("Napisz pytanie do mentoraâ€¦")
    if user_msg:
        _append_chat("user", user_msg)

    # OdpowiedÅº mentora dla najnowszego nieobsÅ‚uÅ¼onego pytania
    pending = [m for m in st.session_state["mentor_chat"] if m["role"] == "user" and not m.get("answered")]
    if pending:
        latest = pending[-1]
        messages = [{"role": "system", "content": st.session_state["mentor_system"]}]
        # doklej przyciÄ™tÄ… historiÄ™ (bez system)
        for m in st.session_state["mentor_chat"]:
            if m["role"] in ("user", "assistant"):
                messages.append({"role": m["role"], "content": m["content"]})
        messages = _trim_history(messages)

        try:
            if online:
                reply = _call_llm(messages=messages, model=model, temperature=float(temperature))
            else:
                raise RuntimeError("OFFLINE")
        except Exception as e:
            log.warning(f"LLM offline/failure, using fallback: {e}")
            reply = _offline_response(latest["content"], _build_context_snapshot())

        _append_chat("assistant", reply)
        latest["answered"] = True  # oznacz jako obsÅ‚uÅ¼one
        # OdÅ›wieÅ¼ UI
        st.rerun()

    # Notatka compliance
    st.markdown("---")
    st.caption("â„¹ï¸ Mentor AI korzysta wyÅ‚Ä…cznie z metadanych i streszczeÅ„ (bez wysyÅ‚ania surowych rekordÃ³w). Traktuj odpowiedzi jako rekomendacje â€” weryfikuj w projekcie.")

# === NAZWA_SEKCJI === WejÅ›cie moduÅ‚u ===
if __name__ == "__main__":
    main()
