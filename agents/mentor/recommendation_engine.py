# agents/mentor/recommendation_engine.py
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  DataGenius PRO Master Enterprise ++++ â€” Recommendation Engine             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
â•‘  Intelligent ML pipeline optimization with enterprise safeguards:          â•‘
â•‘    âœ“ Multi-type recommendations (8 categories)                             â•‘
â•‘    âœ“ Priority-based ranking (4 levels: CRITICAL â†’ LOW)                     â•‘
â•‘    âœ“ Rule-based + AI-powered insights                                      â•‘
â•‘    âœ“ Context-aware suggestions                                             â•‘
â•‘    âœ“ Actionable code examples                                              â•‘
â•‘    âœ“ Impact & effort estimation                                            â•‘
â•‘    âœ“ SHAP-based feature importance analysis                                â•‘
â•‘    âœ“ LLM-powered business insights                                         â•‘
â•‘    âœ“ Comprehensive telemetry                                               â•‘
â•‘    âœ“ Export to Markdown/JSON                                               â•‘
â•‘    âœ“ Defensive error handling                                              â•‘
â•‘    âœ“ Thread-safe operations                                                â•‘
â•‘    âœ“ Versioned output contract                                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from __future__ import annotations

import json
import time
import warnings
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple
from functools import wraps

import numpy as np
import pandas as pd

try:
    from loguru import logger
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

# Domain dependencies
try:
    from core.base_agent import BaseAgent, AgentResult
except ImportError:
    class BaseAgent:
        def __init__(self, name: str, description: str):
            self.name = name
            self.description = description
    
    class AgentResult:
        def __init__(self, agent_name: str):
            self.agent_name = agent_name
            self.data = None
            self.errors = []
            self.warnings = []

try:
    from core.llm_client import get_llm_client
except ImportError:
    get_llm_client = None  # type: ignore


__all__ = [
    "RecommendationType",
    "RecommendationPriority",
    "Recommendation",
    "RecommendationReport",
    "RecommendationEngine",
    "generate_recommendations"
]
__version__ = "5.0-kosmos-enterprise"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION: Enumerations
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RecommendationType(str, Enum):
    """Types of recommendations."""
    FEATURE_ENGINEERING = "feature_engineering"
    MODEL_SELECTION = "model_selection"
    HYPERPARAMETER = "hyperparameter"
    DATA_QUALITY = "data_quality"
    PREPROCESSING = "preprocessing"
    BUSINESS_INSIGHT = "business_insight"
    PERFORMANCE = "performance"
    DEPLOYMENT = "deployment"


class RecommendationPriority(str, Enum):
    """Priority levels for recommendations."""
    CRITICAL = "critical"  # Must address immediately
    HIGH = "high"          # Should address soon
    MEDIUM = "medium"      # Nice to have
    LOW = "low"            # Optional optimization


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION: Data Classes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class Recommendation:
    """
    Single recommendation with comprehensive metadata.
    
    Attributes:
        id: Unique identifier (e.g., REC-0001)
        type: Recommendation category
        priority: Urgency level
        title: Short descriptive title
        description: Detailed explanation
        rationale: Why this matters
        action_items: Concrete steps to implement
        expected_impact: Anticipated improvement
        estimated_effort: Implementation complexity
        code_example: Optional code snippet
        references: External resources
        metadata: Additional context
        timestamp: Creation time
    """
    
    id: str
    type: RecommendationType
    priority: RecommendationPriority
    title: str
    description: str
    rationale: str
    action_items: List[str]
    expected_impact: str
    estimated_effort: str  # "low", "medium", "high"
    code_example: Optional[str] = None
    references: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "id": self.id,
            "type": self.type.value,
            "priority": self.priority.value,
            "title": self.title,
            "description": self.description,
            "rationale": self.rationale,
            "action_items": self.action_items,
            "expected_impact": self.expected_impact,
            "estimated_effort": self.estimated_effort,
            "code_example": self.code_example,
            "references": self.references,
            "metadata": self.metadata,
            "timestamp": self.timestamp.isoformat()
        }


@dataclass
class RecommendationReport:
    """
    Complete recommendation report with analytics.
    
    Attributes:
        recommendations: List of all recommendations
        summary: Executive summary text
        priority_breakdown: Count by priority level
        type_breakdown: Count by recommendation type
        total_count: Total number of recommendations
        telemetry: Performance metrics
        generated_at: Report generation time
    """
    
    recommendations: List[Recommendation]
    summary: str
    priority_breakdown: Dict[str, int]
    type_breakdown: Dict[str, int]
    total_count: int
    telemetry: Dict[str, Any] = field(default_factory=dict)
    generated_at: datetime = field(default_factory=datetime.now)
    version: str = __version__
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "recommendations": [r.to_dict() for r in self.recommendations],
            "summary": self.summary,
            "priority_breakdown": self.priority_breakdown,
            "type_breakdown": self.type_breakdown,
            "total_count": self.total_count,
            "telemetry": self.telemetry,
            "generated_at": self.generated_at.isoformat(),
            "version": self.version
        }
    
    def get_by_priority(self, priority: RecommendationPriority) -> List[Recommendation]:
        """Get recommendations by priority."""
        return [r for r in self.recommendations if r.priority == priority]
    
    def get_by_type(self, rec_type: RecommendationType) -> List[Recommendation]:
        """Get recommendations by type."""
        return [r for r in self.recommendations if r.type == rec_type]
    
    def get_critical_and_high(self) -> List[Recommendation]:
        """Get high-priority recommendations."""
        return [
            r for r in self.recommendations
            if r.priority in (RecommendationPriority.CRITICAL, RecommendationPriority.HIGH)
        ]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION: Utility Functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _timeit(operation_name: str):
    """Decorator for operation timing."""
    def decorator(fn):
        @wraps(fn)
        def wrapper(*args, **kwargs):
            t_start = time.perf_counter()
            try:
                return fn(*args, **kwargs)
            finally:
                elapsed_ms = (time.perf_counter() - t_start) * 1000
                logger.debug(f"â± {operation_name}: {elapsed_ms:.2f}ms")
        return wrapper
    return decorator


def _safe_operation(operation_name: str, default_value: Any = None):
    """Decorator for safe operations with fallback."""
    def decorator(fn):
        @wraps(fn)
        def wrapper(*args, **kwargs):
            try:
                return fn(*args, **kwargs)
            except Exception as e:
                logger.warning(f"âš  {operation_name} failed: {type(e).__name__}: {str(e)[:80]}")
                return default_value
        return wrapper
    return decorator


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION: Main Recommendation Engine
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RecommendationEngine(BaseAgent):
    """
    **RecommendationEngine** â€” Intelligent ML pipeline optimization.
    
    Responsibilities:
      1. Analyze EDA/ML results for improvement opportunities
      2. Generate rule-based recommendations
      3. Create AI-powered insights via LLM
      4. Prioritize actions by impact & urgency
      5. Provide actionable code examples
      6. Estimate implementation effort
      7. Track recommendation telemetry
      8. Export to multiple formats (MD, JSON)
      9. Handle missing data gracefully
      10. Maintain stable output contracts
    
    Features:
      â€¢ 8 recommendation types
      â€¢ 4 priority levels
      â€¢ SHAP-based feature analysis
      â€¢ LLM-powered business insights
      â€¢ Code examples for each recommendation
      â€¢ Impact & effort estimation
      â€¢ Markdown export with emojis
      â€¢ Thread-safe operations
    """
    
    def __init__(self, settings: Optional[Any] = None) -> None:
        """
        Initialize recommendation engine.
        
        Args:
            settings: Optional settings object (for LLM config)
        """
        super().__init__(
            name="RecommendationEngine",
            description="Intelligent ML pipeline optimization recommendations"
        )
        self.settings = settings
        self._log = logger.bind(agent="RecommendationEngine")
        warnings.filterwarnings("ignore")
        
        # Initialize LLM client (optional)
        self.llm_client = self._init_llm_client()
        
        # State
        self.recommendations: List[Recommendation] = []
        self._rec_counter: int = 0
        
        self._log.info("âœ“ RecommendationEngine initialized")
    
    def _init_llm_client(self) -> Optional[Any]:
        """Initialize LLM client for AI-powered recommendations."""
        if get_llm_client is None:
            return None
        
        try:
            return get_llm_client()
        except Exception as e:
            self._log.warning(f"âš  LLM client unavailable: {e}")
            return None
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Main Generation API
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @_timeit("generate_recommendations")
    def generate_recommendations(
        self,
        eda_results: Optional[Dict[str, Any]] = None,
        ml_results: Optional[Dict[str, Any]] = None,
        data: Optional[pd.DataFrame] = None,
        shap_values: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None,
        target_column: Optional[str] = None,
        problem_type: Optional[str] = None
    ) -> RecommendationReport:
        """
        Generate comprehensive recommendations.
        
        Args:
            eda_results: EDA analysis results
            ml_results: ML training results
            data: Original DataFrame
            shap_values: SHAP values for feature importance
            feature_names: List of feature names
            target_column: Target variable name
            problem_type: 'classification' or 'regression'
        
        Returns:
            RecommendationReport with all recommendations
        """
        self._log.info("Generating recommendations...")
        t0 = time.perf_counter()
        
        # Reset state
        self.recommendations = []
        self._rec_counter = 0
        
        # Generate different types of recommendations
        if eda_results:
            self._generate_data_quality_recommendations(eda_results, data)
            self._generate_preprocessing_recommendations(eda_results, data)
        
        if ml_results:
            self._generate_model_recommendations(ml_results, problem_type)
            self._generate_performance_recommendations(ml_results)
        
        if shap_values is not None and feature_names:
            self._generate_feature_recommendations(shap_values, feature_names)
        
        if data is not None:
            self._generate_feature_engineering_recommendations(data, target_column)
        
        # Generate AI-powered recommendations
        if self.llm_client and (eda_results or ml_results):
            self._generate_ai_recommendations(
                eda_results, ml_results, data, problem_type
            )
        
        # Create summary
        summary = self._create_summary()
        
        # Telemetry
        elapsed_ms = (time.perf_counter() - t0) * 1000
        telemetry = {
            "elapsed_ms": round(elapsed_ms, 2),
            "total_recommendations": len(self.recommendations),
            "llm_used": self.llm_client is not None,
            "sources_analyzed": self._count_sources(
                eda_results, ml_results, data, shap_values
            )
        }
        
        # Create report
        report = RecommendationReport(
            recommendations=self.recommendations,
            summary=summary,
            priority_breakdown=self._get_priority_breakdown(),
            type_breakdown=self._get_type_breakdown(),
            total_count=len(self.recommendations),
            telemetry=telemetry
        )
        
        self._log.success(
            f"âœ“ Generated {len(self.recommendations)} recommendations "
            f"in {elapsed_ms:.1f}ms"
        )
        
        return report
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Data Quality Recommendations
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @_safe_operation("data_quality_recommendations")
    def _generate_data_quality_recommendations(
        self,
        eda_results: Dict[str, Any],
        data: Optional[pd.DataFrame]
    ) -> None:
        """Generate recommendations for data quality issues."""
        
        # â”€â”€â”€ Missing Values â”€â”€â”€
        if "missing_values" in eda_results:
            missing = eda_results["missing_values"]
            high_missing = {k: v for k, v in missing.items() if v > 30}
            
            if high_missing:
                self._add_recommendation(
                    type=RecommendationType.DATA_QUALITY,
                    priority=RecommendationPriority.HIGH,
                    title="Wysokie wartoÅ›ci brakujÄ…ce",
                    description=f"Kolumny z >30% brakujÄ…cych wartoÅ›ci: {', '.join(list(high_missing.keys())[:5])}",
                    rationale="Wysokie wartoÅ›ci brakujÄ…ce mogÄ… znaczÄ…co wpÅ‚ynÄ…Ä‡ na jakoÅ›Ä‡ modelu",
                    action_items=[
                        "RozwaÅ¼ usuniÄ™cie kolumn z >50% brakujÄ…cych wartoÅ›ci",
                        "UÅ¼yj zaawansowanej imputacji (KNN, MICE) dla pozostaÅ‚ych",
                        "StwÃ³rz flagÄ™ 'was_missing' dla waÅ¼nych cech",
                        "Przeanalizuj wzorce brakÃ³w (MAR vs MNAR)"
                    ],
                    expected_impact="Poprawa stabilnoÅ›ci modelu o 5-15%",
                    estimated_effort="medium",
                    code_example="""
# KNN Imputation
from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=5)
X_imputed = imputer.fit_transform(X)

# Add missing indicator
X['was_missing'] = X['column'].isna().astype(int)
""",
                    references=[
                        "https://scikit-learn.org/stable/modules/impute.html"
                    ]
                )
        
        # â”€â”€â”€ Outliers â”€â”€â”€
        if "outliers" in eda_results:
            outliers = eda_results["outliers"]
            high_outliers = {k: v for k, v in outliers.items() if v > 5}
            
            if high_outliers:
                self._add_recommendation(
                    type=RecommendationType.PREPROCESSING,
                    priority=RecommendationPriority.MEDIUM,
                    title="Wykryte outliers",
                    description=f"Kolumny z >5% outliers: {', '.join(list(high_outliers.keys())[:5])}",
                    rationale="Outliers mogÄ… zaburzaÄ‡ trening modelu, szczegÃ³lnie dla linear/neural models",
                    action_items=[
                        "Zbadaj czy outliers sÄ… bÅ‚Ä™dami czy prawdziwymi wartoÅ›ciami",
                        "RozwaÅ¼ transformacjÄ™ (log, box-cox) dla skoÅ›nych rozkÅ‚adÃ³w",
                        "UÅ¼yj robust scaling zamiast standardowego",
                        "Dla tree-based models outliers sÄ… mniej problematyczne",
                        "RozwaÅ¼ winsoryzacjÄ™ (cap at percentiles)"
                    ],
                    expected_impact="Lepsza generalizacja modelu",
                    estimated_effort="low",
                    code_example="""
# Robust Scaling
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

# Winsorization
from scipy.stats.mstats import winsorize
X_winsorized = winsorize(X, limits=[0.05, 0.05])
"""
                )
        
        # â”€â”€â”€ Class Imbalance â”€â”€â”€
        if data is not None and "target_distribution" in eda_results:
            dist = eda_results["target_distribution"]
            if isinstance(dist, dict) and len(dist) == 2:
                values = list(dist.values())
                ratio = max(values) / min(values) if min(values) > 0 else float('inf')
                
                if ratio > 3:
                    self._add_recommendation(
                        type=RecommendationType.PREPROCESSING,
                        priority=RecommendationPriority.HIGH,
                        title="Niezbalansowane klasy",
                        description=f"Stosunek klas: {ratio:.1f}:1",
                        rationale="Niezbalansowane klasy prowadzÄ… do biased predictions",
                        action_items=[
                            "UÅ¼yj SMOTE lub ADASYN do oversample minority class",
                            "RozwaÅ¼ class_weight='balanced' w modelu",
                            "UÅ¼yj stratified cross-validation",
                            "Optymalizuj F1-score lub AUC zamiast accuracy",
                            "RozwaÅ¼ undersampling majority class"
                        ],
                        expected_impact="ZnaczÄ…ca poprawa recall dla minority class",
                        estimated_effort="low",
                        code_example="""
# SMOTE
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Class weights
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(class_weight='balanced')
"""
                    )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Preprocessing Recommendations
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @_safe_operation("preprocessing_recommendations")
    def _generate_preprocessing_recommendations(
        self,
        eda_results: Dict[str, Any],
        data: Optional[pd.DataFrame]
    ) -> None:
        """Generate preprocessing recommendations."""
        
        # â”€â”€â”€ High Cardinality Categorical â”€â”€â”€
        if data is not None:
            cat_cols = data.select_dtypes(include=['object', 'category']).columns
            high_card = [col for col in cat_cols if data[col].nunique() > 50]
            
            if high_card:
                self._add_recommendation(
                    type=RecommendationType.FEATURE_ENGINEERING,
                    priority=RecommendationPriority.MEDIUM,
                    title="Wysokie kardynalnoÅ›ci w cechach kategorycznych",
                    description=f"Kolumny: {', '.join(high_card[:5])}",
                    rationale="One-hot encoding wysokich kardynalnoÅ›ci prowadzi do curse of dimensionality",
                    action_items=[
                        "UÅ¼yj target encoding lub frequency encoding",
                        "RozwaÅ¼ grupowanie rzadkich kategorii",
                        "UÅ¼yj embeddings dla bardzo wysokich kardynalnoÅ›ci",
                        "WyprÃ³buj CatBoost (native categorical support)",
                        "RozwaÅ¼ feature hashing dla ekstremalnych przypadkÃ³w"
                    ],
                    expected_impact="Redukcja wymiarowoÅ›ci i lepsza generalizacja",
                    estimated_effort="medium",
                    code_example="""
# Target Encoding
from category_encoders import TargetEncoder

encoder = TargetEncoder()
X_encoded = encoder.fit_transform(X, y)

# Frequency Encoding
freq_map = X['category'].value_counts().to_dict()
X['category_freq'] = X['category'].map(freq_map)
"""
                )
        
        # â”€â”€â”€ Skewed Distributions â”€â”€â”€
        if "distributions" in eda_results:
            skewed = [
                k for k, v in eda_results["distributions"].items()
                if isinstance(v, dict) and abs(v.get("skewness", 0)) > 1
            ]
            
            if skewed:
                self._add_recommendation(
                    type=RecommendationType.PREPROCESSING,
                    priority=RecommendationPriority.MEDIUM,
                    title="SkoÅ›ne rozkÅ‚ady",
                    description=f"Kolumny ze skoÅ›noÅ›ciÄ… >1: {', '.join(skewed[:5])}",
                    rationale="SkoÅ›ne rozkÅ‚ady mogÄ… pogorszyÄ‡ performance linear models",
                    action_items=[
                        "Zastosuj log transform dla right-skewed",
                        "UÅ¼yj Box-Cox lub Yeo-Johnson transform",
                        "RozwaÅ¼ quantile transformation",
                        "Tree-based models sÄ… odporne na skoÅ›noÅ›Ä‡",
                        "UÅ¼yj PowerTransformer z metodÄ… 'yeo-johnson'"
                    ],
                    expected_impact="Lepsza performance dla linear/neural models",
                    estimated_effort="low",
                    code_example="""
# Power Transformation
from sklearn.preprocessing import PowerTransformer

transformer = PowerTransformer(method='yeo-johnson')
X_transformed = transformer.fit_transform(X)

# Log Transform
X_log = np.log1p(X)  # log(1 + x) for zeros
"""
                )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Model Recommendations
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @_safe_operation("model_recommendations")
    def _generate_model_recommendations(
        self,
        ml_results: Dict[str, Any],
        problem_type: Optional[str]
    ) -> None:
        """Generate model selection recommendations."""
        
        if "models" not in ml_results:
            return
        
        models = ml_results["models"]
        
        # Get metric name
        metric = "accuracy" if problem_type == "classification" else "r2"
        
        # Sort models by performance
        sorted_models = sorted(
            models.items(),
            key=lambda x: x[1].get(metric, 0),
            reverse=True
        )
        
        # â”€â”€â”€ Ensemble Recommendation â”€â”€â”€
        if len(sorted_models) >= 2:
            best = sorted_models[0]
            second = sorted_models[1]
            
            gap = abs(best[1].get(metric, 0) - second[1].get(metric, 0))
            
            if gap < 0.02:  # Very close performance
                self._add_recommendation(
                    type=RecommendationType.MODEL_SELECTION,
                    priority=RecommendationPriority.HIGH,
                    title="RozwaÅ¼ ensemble models",
                    description=f"Top 2 modele majÄ… podobnÄ… performance (gap: {gap:.3f})",
                    rationale="Ensemble rÃ³Å¼nych modeli czÄ™sto daje lepsze wyniki przez redukcjÄ™ variance",
                    action_items=[
                        "StwÃ³rz voting classifier/regressor z top 3 models",
                        "WyprÃ³buj stacking z meta-learner (LR, Ridge)",
                        "RozwaÅ¼ blending rÃ³Å¼nych typÃ³w modeli",
                        "UÅ¼yj weighted average based on validation performance",
                        "Eksperymentuj z rÃ³Å¼nymi kombinacjami (bagging + boosting)"
                    ],
                    expected_impact="Poprawa o 2-5% przez redukcjÄ™ variance",
                    estimated_effort="medium",
                    code_example="""
# Voting Classifier
from sklearn.ensemble import VotingClassifier

ensemble = VotingClassifier(
    estimators=[
        ('rf', RandomForestClassifier()),
        ('xgb', XGBClassifier()),
        ('lgb', LGBMClassifier())
    ],
    voting='soft',
    weights=[1, 2, 1]
)
"""
                )
        
        # â”€â”€â”€ Overfitting Detection â”€â”€â”€
        if problem_type == "classification":
            for model_name, metrics in models.items():
                train_acc = metrics.get("train_accuracy", 0)
                val_acc = metrics.get("accuracy", 0)
                
                if train_acc - val_acc > 0.1:  # 10% gap
                    self._add_recommendation(
                        type=RecommendationType.HYPERPARAMETER,
                        priority=RecommendationPriority.CRITICAL,
                        title=f"Overfitting w {model_name}",
                        description=f"Train acc: {train_acc:.3f}, Val acc: {val_acc:.3f} (gap: {train_acc - val_acc:.3f})",
                        rationale="DuÅ¼a rÃ³Å¼nica miÄ™dzy train a validation wskazuje na overfitting",
                        action_items=[
                            "ZwiÄ™ksz regularyzacjÄ™ (L1/L2, dropout)",
                            "Zmniejsz model complexity (fewer trees, lower depth)",
                            "UÅ¼yj early stopping z patience",
                            "Dodaj wiÄ™cej danych treningowych",
                            "UÅ¼yj cross-validation do lepszej estymacji",
                            "RozwaÅ¼ ensembling dla redukcji variance"
                        ],
                        expected_impact="Lepsza generalizacja i stabilnoÅ›Ä‡ w production",
                        estimated_effort="medium",
                        metadata={"model": model_name, "gap": train_acc - val_acc}
                    )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Performance Recommendations
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @_safe_operation("performance_recommendations")
    def _generate_performance_recommendations(
        self,
        ml_results: Dict[str, Any]
    ) -> None:
        """Generate performance optimization recommendations."""
        
        if "training_time" in ml_results:
            train_time = ml_results["training_time"]
            
            if train_time > 300:  # 5 minutes
                self._add_recommendation(
                    type=RecommendationType.PERFORMANCE,
                    priority=RecommendationPriority.MEDIUM,
                    title="DÅ‚ugi czas treningu",
                    description=f"Czas treningu: {train_time:.1f}s ({train_time/60:.1f} min)",
                    rationale="DÅ‚ugi training time utrudnia iteracje i eksperymenty",
                    action_items=[
                        "UÅ¼yj feature selection do redukcji wymiarowoÅ›ci",
                        "RozwaÅ¼ sampling duÅ¼ych datasets (stratified)",
                        "UÅ¼yj early stopping z patience=10-20",
                        "Optymalizuj hyperparametry (max_depth, n_estimators)",
                        "RozwaÅ¼ distributed training (Dask, Ray)",
                        "UÅ¼yj GPU acceleration (XGBoost, LightGBM)"
                    ],
                    expected_impact="Szybsze iteracje (5-10x speedup moÅ¼liwy)",
                    estimated_effort="high",
                    code_example="""
# Feature Selection
from sklearn.feature_selection import SelectFromModel

selector = SelectFromModel(RandomForestClassifier(), threshold='median')
X_selected = selector.fit_transform(X, y)

# Early Stopping
model = XGBClassifier(early_stopping_rounds=20)
model.fit(X_train, y_train, eval_set=[(X_val, y_val)])
"""
                )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Feature Importance Recommendations
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    @_safe_operation("feature_recommendations")
    def _generate_feature_recommendations(
        self,
        shap_values: np.ndarray,
        feature_names: List[str]
    ) -> None:
        """Generate feature importance recommendations."""
        
        # Calculate mean absolute SHAP values
        mean_shap = np.abs(shap_values).mean(axis=0)
        
        # Find low importance features
        threshold = np.percentile(mean_shap, 25)
        low_importance = [
            feature_names[i] for i, val in enumerate(mean_shap)
            if val < threshold
        ]
        
        if low_importance:
            self._add_recommendation(
                type=RecommendationType.FEATURE_ENGINEERING,
                priority=RecommendationPriority.MEDIUM,
                title="Cechy o niskiej waÅ¼noÅ›ci",
                description=f"Wykryto {len(low_importance)} cech o niskiej waÅ¼noÅ›ci (bottom 25%)",
                rationale="UsuniÄ™cie nieistotnych cech moÅ¼e poprawiÄ‡ generalizacjÄ™ i czytelnoÅ›Ä‡",
                action_items=[
                    "Przeanalizuj cechy o najniÅ¼szej waÅ¼noÅ›ci",
                    "RozwaÅ¼ usuniÄ™cie cech z SHAP < 0.01",
                    "SprawdÅº czy cechy sÄ… skorelowane z waÅ¼niejszymi",
                    "UÅ¼yj feature selection (RFE, SelectFromModel)",
                    "Zachowaj domain knowledge features mimo niskiej waÅ¼noÅ›ci"
                ],
                expected_impact="Prostszy model, lepsza interpretability, szybszy trening",
                estimated_effort="low",
                metadata={
                    "low_importance_features": low_importance[:10],
                    "threshold": float(threshold)
                },
                code_example="""
# Feature
Selection based on importance
from sklearn.feature_selection import SelectFromModel
Using Random Forest importances
selector = SelectFromModel(
RandomForestClassifier(),
threshold=0.01  # Remove features with importance < 0.01
)
X_selected = selector.fit_transform(X, y)
Get selected features
selected_features = X.columns[selector.get_support()]
"""
)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Feature Engineering Recommendations
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@_safe_operation("feature_engineering_recommendations")
def _generate_feature_engineering_recommendations(
    self,
    data: pd.DataFrame,
    target_column: Optional[str]
) -> None:
    """Generate feature engineering recommendations."""
    
    # â”€â”€â”€ Datetime Features â”€â”€â”€
    date_cols = [
        col for col in data.columns
        if pd.api.types.is_datetime64_any_dtype(data[col])
    ]
    
    if date_cols:
        self._add_recommendation(
            type=RecommendationType.FEATURE_ENGINEERING,
            priority=RecommendationPriority.HIGH,
            title="Wykorzystaj cechy temporalne",
            description=f"Znaleziono kolumny datetime: {', '.join(date_cols)}",
            rationale="Datetime features zawierajÄ… ukryte wzorce (sezonowoÅ›Ä‡, trendy, cykle)",
            action_items=[
                "WyodrÄ™bnij: year, month, day, day_of_week, hour, minute",
                "StwÃ³rz: is_weekend, is_holiday, is_business_hour, season",
                "Oblicz: time_since_event, days_until_event, time_diff",
                "UÅ¼yj cyclic encoding dla periodic features (sin/cos)",
                "Agreguj temporal features (rolling means, lag features)"
            ],
            expected_impact="ZnaczÄ…ca poprawa dla time-series data (10-30%)",
            estimated_effort="medium",
            code_example="""
Extract date features
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6])
df['hour'] = df['date'].dt.hour
df['is_business_hour'] = df['hour'].between(9, 17)
Cyclic encoding for periodic features
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
Rolling features
df['rolling_mean_7d'] = df.groupby('id')['value'].transform(
lambda x: x.rolling(window=7, min_periods=1).mean()
)
""",
references=[
"https://pandas.pydata.org/docs/user_guide/timeseries.html"
]
)
    # â”€â”€â”€ Numeric Binning â”€â”€â”€
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 3:
        self._add_recommendation(
            type=RecommendationType.FEATURE_ENGINEERING,
            priority=RecommendationPriority.LOW,
            title="RozwaÅ¼ binning numeric features",
            description="Binning moÅ¼e odkryÄ‡ non-linear relationships",
            rationale="Discretization moÅ¼e pomÃ³c modelom liniowym capture non-linearity",
            action_items=[
                "UÅ¼yj KBinsDiscretizer dla continuous features",
                "StwÃ³rz percentile-based bins (quartiles, deciles)",
                "Kombinuj original + binned features",
                "SzczegÃ³lnie przydatne dla linear/logistic models",
                "RozwaÅ¼ equal-width vs equal-frequency binning"
            ],
            expected_impact="Lepsza performance dla linear models (5-10%)",
            estimated_effort="low",
            code_example="""
KBins Discretization
from sklearn.preprocessing import KBinsDiscretizer
binner = KBinsDiscretizer(
n_bins=5,
encode='ordinal',
strategy='quantile'
)
X_binned = binner.fit_transform(X[['age', 'income']])
Manual binning with pd.cut
df['age_group'] = pd.cut(
df['age'],
bins=[0, 18, 30, 45, 60, 100],
labels=['<18', '18-30', '30-45', '45-60', '60+']
)
"""
)
    # â”€â”€â”€ Interaction Features â”€â”€â”€
    if len(numeric_cols) >= 2:
        self._add_recommendation(
            type=RecommendationType.FEATURE_ENGINEERING,
            priority=RecommendationPriority.MEDIUM,
            title="StwÃ³rz interaction features",
            description="Kombinacje cech mogÄ… capture complex relationships",
            rationale="Interakcje miÄ™dzy cechami czÄ™sto majÄ… wysokÄ… wartoÅ›Ä‡ predykcyjnÄ…",
            action_items=[
                "StwÃ³rz polynomial features (xâ‚ * xâ‚‚, xâ‚Â²)",
                "Ratio features (xâ‚ / xâ‚‚) dla zwiÄ…zanych miar",
                "Conditional features (if-then logic)",
                "Agregacje grupowe (group by + mean/sum/count)",
                "UÅ¼ywaj domain knowledge do wyboru par"
            ],
            expected_impact="ZnaczÄ…ca poprawa dla linear models, marginalna dla trees",
            estimated_effort="medium",
            code_example="""
Polynomial features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X[['feature1', 'feature2']])
Ratio features
df['price_per_sqft'] = df['price'] / df['square_feet']
df['income_to_debt'] = df['income'] / (df['debt'] + 1)
Group aggregations
df['avg_price_by_category'] = df.groupby('category')['price'].transform('mean')
"""
)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI-Powered Recommendations
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@_safe_operation("ai_recommendations")
def _generate_ai_recommendations(
    self,
    eda_results: Optional[Dict[str, Any]],
    ml_results: Optional[Dict[str, Any]],
    data: Optional[pd.DataFrame],
    problem_type: Optional[str]
) -> None:
    """Generate AI-powered recommendations using LLM."""
    
    if not self.llm_client:
        return
    
    try:
        # Prepare context
        context = self._prepare_llm_context(
            eda_results, ml_results, data, problem_type
        )
        
        prompt = f"""JesteÅ› ekspertem ML/Data Science. Przeanalizuj poniÅ¼sze wyniki i wygeneruj 2-3 najwaÅ¼niejsze, actionable rekomendacje.
KONTEKST ANALIZY:
{context}
Wygeneruj rekomendacje w formacie JSON (czyste JSON, bez markdown):
[
{{
"title": "KrÃ³tki, konkretny tytuÅ‚",
"description": "SzczegÃ³Å‚owy opis problemu i rozwiÄ…zania (2-3 zdania)",
"priority": "high",
"action_items": ["Konkretna akcja 1", "Konkretna akcja 2", "Konkretna akcja 3"],
"expected_impact": "Konkretny, mierzalny wpÅ‚yw"
}}
]
WYMAGANIA:

Skup siÄ™ na najbardziej wpÅ‚ywowych improvement
Podaj konkretne, wykonalne dziaÅ‚ania
WskaÅ¼ business value i ROI
Priorytetyzuj quick wins vs long-term gains
UÅ¼ywaj polskiego jÄ™zyka

Wygeneruj TYLKO JSON, bez Å¼adnego dodatkowego tekstu."""
        response = self.llm_client.generate(
            prompt=prompt,
            temperature=0.7,
            max_tokens=1500
        )
        
        # Extract content
        content = getattr(response, 'content', str(response))
        
        # Clean response (remove markdown if present)
        content = content.strip()
        if content.startswith('```'):
            lines = content.split('\n')
            content = '\n'.join(lines[1:-1] if len(lines) > 2 else lines)
        
        # Parse JSON
        recommendations = json.loads(content)
        
        # Priority mapping
        priority_map = {
            "critical": RecommendationPriority.CRITICAL,
            "high": RecommendationPriority.HIGH,
            "medium": RecommendationPriority.MEDIUM,
            "low": RecommendationPriority.LOW
        }
        
        # Add recommendations
        for rec in recommendations:
            if not isinstance(rec, dict):
                continue
            
            self._add_recommendation(
                type=RecommendationType.BUSINESS_INSIGHT,
                priority=priority_map.get(
                    rec.get("priority", "medium"),
                    RecommendationPriority.MEDIUM
                ),
                title=rec.get("title", "AI Recommendation"),
                description=rec.get("description", ""),
                rationale="AI-powered insight based on comprehensive analysis",
                action_items=rec.get("action_items", []),
                expected_impact=rec.get("expected_impact", "Varies"),
                estimated_effort="varies",
                metadata={"source": "ai_powered", "llm": "claude"}
            )
        
        self._log.info(f"âœ“ Generated {len(recommendations)} AI-powered recommendations")
    
    except json.JSONDecodeError as e:
        self._log.warning(f"âš  Could not parse LLM JSON response: {e}")
    except Exception as e:
        self._log.warning(f"âš  Could not generate AI recommendations: {e}")

def _prepare_llm_context(
    self,
    eda_results: Optional[Dict[str, Any]],
    ml_results: Optional[Dict[str, Any]],
    data: Optional[pd.DataFrame],
    problem_type: Optional[str]
) -> str:
    """Prepare context for LLM."""
    parts: List[str] = []
    
    if problem_type:
        parts.append(f"â€¢ Typ problemu: {problem_type}")
    
    if data is not None:
        parts.append(f"â€¢ Dataset: {len(data):,} wierszy, {len(data.columns)} kolumn")
    
    if eda_results:
        if "missing_values" in eda_results:
            n_missing = len(eda_results["missing_values"])
            parts.append(f"â€¢ Braki danych: {n_missing} kolumn")
        
        if "outliers" in eda_results:
            n_outliers = len(eda_results["outliers"])
            parts.append(f"â€¢ Outliers wykryte w {n_outliers} kolumnach")
        
        if "correlations" in eda_results:
            parts.append("â€¢ Analiza korelacji dostÄ™pna")
    
    if ml_results:
        if "models" in ml_results:
            n_models = len(ml_results["models"])
            parts.append(f"â€¢ Przetestowano {n_models} modeli")
            
            # Best score
            metric = "accuracy" if problem_type == "classification" else "r2"
            scores = [
                m.get(metric, 0)
                for m in ml_results["models"].values()
            ]
            if scores:
                best_score = max(scores)
                parts.append(f"â€¢ Najlepszy wynik: {best_score:.3f}")
        
        if "training_time" in ml_results:
            time_s = ml_results["training_time"]
            parts.append(f"â€¢ Czas treningu: {time_s:.1f}s")
    
    return "\n".join(parts) if parts else "Brak szczegÃ³Å‚owego kontekstu"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper Methods
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _add_recommendation(
    self,
    type: RecommendationType,
    priority: RecommendationPriority,
    title: str,
    description: str,
    rationale: str,
    action_items: List[str],
    expected_impact: str,
    estimated_effort: str,
    code_example: Optional[str] = None,
    references: Optional[List[str]] = None,
    metadata: Optional[Dict[str, Any]] = None
) -> None:
    """Add recommendation to the list."""
    self._rec_counter += 1
    
    rec = Recommendation(
        id=f"REC-{self._rec_counter:04d}",
        type=type,
        priority=priority,
        title=title,
        description=description,
        rationale=rationale,
        action_items=action_items,
        expected_impact=expected_impact,
        estimated_effort=estimated_effort,
        code_example=code_example,
        references=references or [],
        metadata=metadata or {}
    )
    
    self.recommendations.append(rec)

def _create_summary(self) -> str:
    """Create executive summary."""
    total = len(self.recommendations)
    critical = len([
        r for r in self.recommendations
        if r.priority == RecommendationPriority.CRITICAL
    ])
    high = len([
        r for r in self.recommendations
        if r.priority == RecommendationPriority.HIGH
    ])
    
    summary = f"""Wygenerowano {total} rekomendacji do optymalizacji ML pipeline.
PRIORYTETY:

CRITICAL: {critical} rekomendacji wymagajÄ…cych natychmiastowej uwagi
HIGH: {high} rekomendacji z wysokim priorytetem
PozostaÅ‚e: {total - critical - high} rekomendacji do rozwaÅ¼enia

KLUCZOWE OBSZARY:

Data quality & preprocessing
Feature engineering
Model optimization
Performance improvements

Implementacja tych rekomendacji moÅ¼e znaczÄ…co poprawiÄ‡ performance i stabilnoÅ›Ä‡ modelu."""
    return summary.strip()

def _get_priority_breakdown(self) -> Dict[str, int]:
    """Get count by priority."""
    breakdown = defaultdict(int)
    for rec in self.recommendations:
        breakdown[rec.priority.value] += 1
    return dict(breakdown)

def _get_type_breakdown(self) -> Dict[str, int]:
    """Get count by type."""
    breakdown = defaultdict(int)
    for rec in self.recommendations:
        breakdown[rec.type.value] += 1
    return dict(breakdown)

def _count_sources(
    self,
    eda_results: Optional[Dict[str, Any]],
    ml_results: Optional[Dict[str, Any]],
    data: Optional[pd.DataFrame],
    shap_values: Optional[np.ndarray]
) -> int:
    """Count data sources analyzed."""
    count = 0
    if eda_results:
        count += 1
    if ml_results:
        count += 1
    if data is not None:
        count += 1
    if shap_values is not None:
        count += 1
    return count

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Export Methods
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def export_to_markdown(self, report: RecommendationReport) -> str:
    """Export recommendations to Markdown format."""
    md: List[str] = []

    # Header
    md.append("# ðŸŽ¯ ML Pipeline Recommendations\n")
    md.append(f"**Generated:** {report.generated_at.strftime('%Y-%m-%d %H:%M')}\n")
    md.append(f"**Total Recommendations:** {report.total_count}\n")
    md.append(f"**Version:** {report.version}\n")
    md.append("---\n")

    # Summary
    md.append("## ðŸ“‹ Executive Summary\n")
    md.append(report.summary)
    md.append("\n---\n")

    # Breakdowns
    md.append("## ðŸ“Š Breakdown\n")
    md.append("**By Priority:**\n")
    for priority, count in sorted(report.priority_breakdown.items()):
        md.append(f"- {priority}: {count}\n")

    md.append("\n**By Type:**\n")
    for rec_type, count in sorted(report.type_breakdown.items()):
        md.append(f"- {rec_type}: {count}\n")
    md.append("\n---\n")

    # Recommendations by priority
    priority_order = [
        RecommendationPriority.CRITICAL,
        RecommendationPriority.HIGH,
        RecommendationPriority.MEDIUM,
        RecommendationPriority.LOW,
    ]

    emoji_map = {
        RecommendationPriority.CRITICAL: "ðŸš¨",
        RecommendationPriority.HIGH: "âš ï¸",
        RecommendationPriority.MEDIUM: "â„¹ï¸",
        RecommendationPriority.LOW: "ðŸ’¡",
    }

    for priority in priority_order:
        recs = report.get_by_priority(priority)
        if not recs:
            continue

        md.append(f"\n## {emoji_map[priority]} {priority.value.upper()} Priority\n")

        for rec in recs:
            md.append(f"\n### {rec.id}: {rec.title}\n")
            md.append(
                f"**Type:** `{rec.type.value}` | "
                f"**Effort:** `{rec.estimated_effort}` | "
                f"**Impact:** {rec.expected_impact}\n\n"
            )
            md.append(f"{rec.description}\n\n")
            md.append(f"**Rationale:** {rec.rationale}\n\n")

            md.append("**Action Items:**\n")
            for item in rec.action_items:
                md.append(f"- {item}\n")

            if rec.code_example:
                md.append("\n**Code Example:**\n```python\n")
                md.append(f"{rec.code_example}\n")
                md.append("```\n")

            if rec.references:
                md.append("\n**References:**\n")
                for ref in rec.references:
                    md.append(f"- {ref}\n")

            md.append("\n---\n")

    return "".join(md)


def export_to_json(self, report: RecommendationReport) -> str:
    """Export recommendations to JSON format."""
    return json.dumps(report.to_dict(), indent=2, ensure_ascii=False)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SECTION: Convenience Function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_recommendations(
    eda_results: Optional[Dict[str, Any]] = None,
    ml_results: Optional[Dict[str, Any]] = None,
    data: Optional[pd.DataFrame] = None,
    **kwargs: Any,
) -> RecommendationReport:
    """
    Convenience function to generate recommendations.

    Example:
        report = generate_recommendations(
            eda_results=eda_results,
            ml_results=ml_results,
            data=df,
            problem_type="classification",
        )
        # Export:
        # markdown = engine.export_to_markdown(report)
        # json_str = engine.export_to_json(report)

    Args:
        eda_results: EDA analysis results.
        ml_results: ML training results.
        data: Original DataFrame.
        **kwargs: Additional parameters (e.g., shap_values, feature_names).

    Returns:
        RecommendationReport with all recommendations.
    """
    engine = RecommendationEngine()
    return engine.generate_recommendations(
        eda_results=eda_results,
        ml_results=ml_results,
        data=data,
        **kwargs,
    )
